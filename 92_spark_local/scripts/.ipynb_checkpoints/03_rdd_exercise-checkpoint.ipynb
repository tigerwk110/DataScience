{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD编程练习题 \n",
    "说明：下面题目中给出的所有数据都要专程rdd，请不要直接使用普通的python方法进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'RDD Exercise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：将列表[3,6,1,2,3,4]转换为rdd，将该rdd的每一个元素求平方，然后进行排序。<br>\n",
    "要求：分别用lambda的方式和调用外部函数的方式求rdd中每个元素的平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 9, 16, 36]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([3,6,1,2,3,4])\n",
    "rdd_square = rdd.map(lambda x: x ** 2)\n",
    "print(sorted(rdd_square.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：过滤掉['apple', 'egg', 'banana']中不含'a'的单词，并打印出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'banana']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['apple', 'egg', 'banana'])\n",
    "rdd.filter(lambda x: 'a' in x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：打印rdd [\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\"] 中所有的单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：打印rdd [\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\"]中元素的个数以及每个元素出现的个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\"])\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'spark': 2, 'hadoop': 2, 'hive': 1, 'pig': 1, 'cassandra': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark: 2\n",
      "hadoop: 2\n",
      "hive: 1\n",
      "pig: 1\n",
      "cassandra: 1\n"
     ]
    }
   ],
   "source": [
    "for key, value in rdd.countByValue().items():\n",
    "    print('{}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：'data/nasa_19950701.tsv' 中包含nasa 1995年7月1日某一台apache服务器的10000行log。<br>\n",
    "'data/nasa_19950801.tsv' 中包含nasa 1995年8月1日某一台apache服务器的10000行log。<br>\n",
    "创建一个Spark程序，创建RDD，查找哪一个host同时出现在这两天<br>\n",
    "保存RDD到'data/nasa_logs_same_hosts.csv'<br>\n",
    "\n",
    "输出的例子:\n",
    "vagrant.vf.mmc.com<br>\n",
    "www-a1.proxy.aol.com<br>\n",
    "...    <br>\n",
    "提示：记得删除log日志的第一行，因为第一行是titile信息。<br>\n",
    "host    logname    time    method    url    response    bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "julyFirstLogs = sc.textFile('../data/nasa_19950701.tsv')\n",
    "augustFirstLogs = sc.textFile('../data/nasa_19950801.tsv')\n",
    "\n",
    "julyFirstHosts = julyFirstLogs.map(lambda line: line.split('\\t')[0])\n",
    "augustFirstHosts = augustFirstLogs.map(lambda line: line.split('\\t')[0])\n",
    "# 使用rdd1.intersection(rdd2)取交集，是一个转换操作\n",
    "intersection = julyFirstHosts.intersection(augustFirstHosts)\n",
    "\n",
    "cleanedHostIntersection = intersection.filter(lambda host: host != \"host\")\n",
    "# 保存文件\n",
    "cleanedHostIntersection.saveAsTextFile(\"../data/nasa_logs_same_hosts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：请查看生成了几个文件，并回答为什么生成了这么多文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**题目**：从这两份log文件中随机选取选取1%作为样本保存起来。记得删除头文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNotHeader(line: str):\n",
    "    return not (line.startswith(\"host\") and \"bytes\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "julyFirstLogs = sc.textFile('../data/nasa_19950701.tsv')\n",
    "augustFirstLogs = sc.textFile('../data/nasa_19950801.tsv')\n",
    "\n",
    "aggregatedLogLines = julyFirstLogs.union(augustFirstLogs)\n",
    "\n",
    "cleanLogLines = aggregatedLogLines.filter(isNotHeader)\n",
    "# sample(withReplacement, fraction)转换操作\n",
    "# withReplacement ：取样后是否将元素放回。 \n",
    "#    true:元素放回 ，返回的子集会有重复，可以被多次抽样；\n",
    "#    false:元素不放回 ，返回的子集没有重复\n",
    "# fraction：期望样本的大小作为RDD大小的一部分， \t\n",
    "sample = cleanLogLines.sample(withReplacement = True, fraction = 0.1)\n",
    "\n",
    "sample.saveAsTextFile('../data/sample_nasa_logs.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
