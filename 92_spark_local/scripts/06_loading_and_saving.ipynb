{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据的读取与存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a SparkSession in Python\n",
    "from pyspark.sql import SparkSession\n",
    "# getOrCreate()如果存在则获取，如果不存在则创建\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Spark Reading and Loading\").getOrCreate() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本文件\n",
    "使用SparkContext中的textFile()函数，就可以读取一个文本文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone,8000', 'iwhach,4000', 'pro,10000', 'pro,10000']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd = spark.sparkContext.textFile('./data/sales/sales_1')\n",
    "sale_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果多个输入文件存放在一个目录下，仍然可以用textFile将所有数据都读入的rdd中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone,8000',\n",
       " 'iwhach,4000',\n",
       " 'ipad,3000',\n",
       " 'ipad,3000',\n",
       " 'iphone,8000',\n",
       " 'iwhach,4000',\n",
       " 'pro,10000',\n",
       " 'pro,10000']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd = spark.sparkContext.textFile('./data/sales')\n",
    "sale_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于多个文件存放在一个路目录下，我们又想知道哪些数据来自于哪个文件，<br>\n",
    "我们可以使用wholeTextFiles()把所有目录下所有数据读入到一个键值对RDD中。<br>\n",
    "key为文件名<br>\n",
    "但最好用于小文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/home/jovyan/work/scripts/data/sales/sales_2',\n",
       "  'iphone,8000\\niwhach,4000\\nipad,3000\\nipad,3000\\n'),\n",
       " ('file:/home/jovyan/work/scripts/data/sales/sales_1',\n",
       "  'iphone,8000\\niwhach,4000\\npro,10000\\npro,10000\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd = spark.sparkContext.wholeTextFiles('./data/sales')\n",
    "sale_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用RDD的saveAsTextFile()方法，可以将RDD保存到对应的路径中<br>\n",
    "saveAsTextFile()接受一个参数，spark将该参数当作**路径** ,会在该路径下生成多个文件，<br>\n",
    "这样，spark就会在多个路径下并行的输出了，spark并不能控制哪一部分输出到哪个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_rdd.saveAsTextFile('./data/sale_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取json的一种方式是，按照读普通文本的方式将数据都加载到rdd中，使用python、scala或java中的json解释器进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Michael'},\n",
       " {'name': 'Andy', 'age': 30},\n",
       " {'name': 'Justin', 'age': 19}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "sale_rdd = spark.sparkContext.textFile('./data/people.json')\n",
    "sale_rdd.map(lambda x: json.loads(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "或者先加载到DataFrame中再转换为RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json(\"./data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.rdd.filter(lambda x: x['name'] == 'Andy').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=30, name='Andy')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.rdd.filter(lambda x: x[1] == 'Andy').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将RDD导出到json文件，可以使用python、scala或者java中内置的json处理方法，将数据转换为json格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone,8000', 'iwhach,4000', 'pro,10000', 'pro,10000']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd = spark.sparkContext.textFile('./data/sales/sales_1')\n",
    "sale_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'iphone', 'price': '8000'},\n",
       " {'name': 'iwhach', 'price': '4000'},\n",
       " {'name': 'pro', 'price': '10000'},\n",
       " {'name': 'pro', 'price': '10000'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "def convert_to_dict(x):\n",
    "    res = {}\n",
    "    res['name'] = x.split(',')[0]\n",
    "    res['price'] = x.split(',')[1]\n",
    "    return res\n",
    "    \n",
    "sale_rdd_json = sale_rdd.map(convert_to_dict)\n",
    "sale_rdd_json.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_rdd_json.map(lambda x: json.dumps(x)).saveAsTextFile('./data/sale_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name,age,job', 'Jorge,30,Developer', 'Bob,32,Developer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd = spark.sparkContext.textFile('./data/people.csv')\n",
    "sale_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['name', 'age', 'job'],\n",
       " ['Jorge', '30', 'Developer'],\n",
       " ['Bob', '32', 'Developer']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd.map(lambda x: x.split(',')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取到DataFrame中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_rdd = spark.read.csv('./data/people.csv', sep = ',', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sale_rdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Jorge', age='30', job='Developer'),\n",
       " Row(name='Bob', age='32', job='Developer')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_rdd.rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
