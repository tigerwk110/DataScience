{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 键值对RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'Pair RDD Programming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 创建 \n",
    "构建pair rdd的方式在不同语言中有所不同。<br>\n",
    "Python中，为了让提取键之后的数据能够在函数中应用，需要返回一个由二元组组成的RDD。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 1),\n",
       " ('Spark', 1),\n",
       " ('Python', 1),\n",
       " ('Spark', 1),\n",
       " ('MachineLearning', 1),\n",
       " ('MachineLearning', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Python\", \"Spark\", \"MachineLearning\", \"MachineLearning\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word : (word, 1))\n",
    "pairRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚合操作 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey(func)\n",
    "reduceByKey(func)的功能是，使用func函数合并具有相同键的值。<br>\n",
    "例如如下代码，对具有相同key的的value求合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', 1), ('Spark', 2), ('Python', 1), ('MachineLearning', 2)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Python\", \"Spark\", \"MachineLearning\", \"MachineLearning\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word : (word, 1))\n",
    "pairRDD.reduceByKey(lambda a, b : a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用reduceByKey与mapValues求key对应的均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', (0, 1)), ('Spark', (5, 2)), ('Python', (1, 1))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "# 转换为pairRDD形式\n",
    "pairRDD = rdd.map(lambda t: (t[0], t[1]))\n",
    "# mapValues(func)是对每个value进行操作\n",
    "sum_num = pairRDD.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "sum_num.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 2.5, 1.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_num.map(lambda x: x[1][0] / x[1][1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey()\n",
    "groupByKey()的功能是，对具有相同键的值进行分组。\n",
    "例如对[('Hadoop', 1),\n",
    " ('Spark', 1),\n",
    " ('Python', 1),\n",
    " ('Spark', 1),\n",
    " ('MachineLearning', 1),\n",
    " ('MachineLearning', 1)]\n",
    " 进行分组的结果是[('Hadoop', (1)), ('Spark', (1, 1)), ('Python', (1)), ('MachineLearning', (1, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hadoop', <pyspark.resultiterable.ResultIterable at 0x7feb00175668>),\n",
       " ('Spark', <pyspark.resultiterable.ResultIterable at 0x7feb00175dd8>),\n",
       " ('Python', <pyspark.resultiterable.ResultIterable at 0x7feb001750f0>),\n",
       " ('MachineLearning',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x7feb00175cf8>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Python\", \"Spark\", \"MachineLearning\", \"MachineLearning\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word : (word, 1))\n",
    "pairRDD.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 连接\n",
    "join是内连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', (3, 3)), ('Spark', (2, 3)), ('Hadoop', (0, 0))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1 = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "rdd_2 = sc.parallelize([('Hadoop', 0), ('Spark', 3)])\n",
    "rdd_1.join(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "leftOuterJoin() 是左外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', (3, 3)),\n",
       " ('Spark', (2, 3)),\n",
       " ('Python', (1, None)),\n",
       " ('Hadoop', (0, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1 = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "rdd_2 = sc.parallelize([('Hadoop', 0), ('Spark', 3)])\n",
    "rdd_1.leftOuterJoin(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rightOuterJoin() 是右外连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', (3, 3)), ('Spark', (2, 3)), ('Java', (None, 3)), ('Hadoop', (0, 0))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_1 = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "rdd_2 = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Java', 3)])\n",
    "rdd_1.rightOuterJoin(rdd_2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排序\n",
    "我们使用sortByKey()对键值对RDD进行排序操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 3), ('Spark', 2), ('Python', 1), ('Hadoop', 0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "# ascending = True, 升序\n",
    "# ascending = False, 降序\n",
    "rdd.sortByKey(ascending=False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Spark', 3), ('Spark', 2), ('Python', 1), ('Hadoop', 0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同样可以传递函数\n",
    "# 按照字符串的顺序排序\n",
    "rdd.sortByKey(ascending=False, keyfunc = lambda x: str(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keys()\n",
    "keys()会把键值对RDD中的key返回形成一个新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hadoop', 'Spark', 'Python', 'Spark', 'MachineLearning', 'MachineLearning']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Python\", \"Spark\", \"MachineLearning\", \"MachineLearning\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word : (word, 1))\n",
    "pairRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### values()\n",
    "values()会把键值对RDD中的value返回形成一个新的RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [\"Hadoop\", \"Spark\", \"Python\", \"Spark\", \"MachineLearning\", \"MachineLearning\"]\n",
    "rdd = sc.parallelize(list)\n",
    "pairRDD = rdd.map(lambda word : (word, 1))\n",
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 2.5, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "pairRDD = rdd.map(lambda t: (t[0], t[1]))\n",
    "pairRDD.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda x: x[1][0] / x[1][1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 行动操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## countByKey() 对每个键的值的个数进行计数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'Hadoop': 1, 'Spark': 2, 'Python': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "pairRDD = rdd.map(lambda t: (t[0], t[1]))\n",
    "pairRDD.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lookup() 返回给定键对应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('Hadoop', 0), ('Spark', 3), ('Python', 1), ('Spark', 2)])\n",
    "pairRDD = rdd.map(lambda t: (t[0], t[1]))\n",
    "pairRDD.lookup('Spark')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
