{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Tensorflow (低级API)构建CNN对MINIST数据进行分类\n",
    "\n",
    "构建CNN来对MNIST手写数字进行识别来探索Tensorflow。<br>\n",
    "\n",
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, mnist_y = train\n",
    "\n",
    "mnist_y = tf.keras.utils.to_categorical(mnist_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAACEUlEQVR4nO3aT6hNURTH8c+TkJDQy5uQSMifV6+UgZmYMFRkLDMZkDKQklKe8idTQ2WAoRiYGRkwEMqfEIYvBiYkBufe6zyue88+vVq9rN/gds9Ze6++/e5q3dVuk0qlUqlUKvW/a6R8yybYi8PwCJ50Y5fwrSjbnHKAmVUChAMUFuERXIBF/eO78KAoY7gDCVBYA8vwDEb7xz/jANxvmjHcgQQIB5hbtnwKZ2ASC+E9rOrGl2IPWYQJUKAWIxl4jG3wFDbXQ+vgTdNE4Q4kQGEj6ukcTsH4X6H5RYnCHUiAcIC2jQhWwj3YUn99C/Y3TRLuQAK0bUSHsJU//ofAw6JE4Q4kQDhAYSPagNtUY88/CjgnolkGUNiINmLNkH3H4GjTjOEOJEA4QGER3sFJOI8F/deMFWUMdyABwgFajGRX4KXqROp3lquwpDhbuAMJ0HYsvzvtaQRr4bTOicFqvGuSKNyBBAgHaFuE0zVPVX/gO/xoujXcgQSYmRo4W3+4Dh+abg13IAHCAQadDyynU1I3caP/ojF4rjYM5fnALAMY1Iguwz5Yj486H69govPWCWq//0V8KgIIdyABwgEGNaIdVBcFqm/wVucSy04s7q38iRewHV+LAMIdSIBwgKGn5ZPwGtcGLJrCinYA4Q4kwNCx/DjVlYDeHcpxONh9+gK72wOEO5AA4QCpVCqVSqVSqV/NyzjyU/espwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7F7927889940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "Image.fromarray(mnist_x[10]).resize((128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建Softmax 回归模型\n",
    "在这一节中我们将建立一个拥有一个线性层的softmax回归模型。\n",
    "\n",
    "使用TensorFlow之前，首先导入它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建一个多层卷积网络\n",
    "\n",
    "在MNIST上只有91%正确率，实在太糟糕。在这个小节里，我们用一个稍微复杂的模型：卷积神经网络来改善效果。这会达到大概99.2%的准确率。虽然不是最高，但是还是比较让人满意。\n",
    "\n",
    "# 权重初始化\n",
    "\n",
    "为了创建这个模型，我们需要创建大量的权重和偏置项。\n",
    "\n",
    "这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。\n",
    "\n",
    "由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。\n",
    "\n",
    "为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积和池化\n",
    "\n",
    "TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？\n",
    "\n",
    "在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。\n",
    "\n",
    "我们的池化用简单传统的2x2大小的模板做max pooling。\n",
    "\n",
    "为了代码更简洁，我们把这部分抽象成一个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 占位符\n",
    "我们通过为输入图像和目标输出类别创建节点，来开始构建计算图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", shape=[None, 28, 28, 1])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层卷积\n",
    "现在我们可以开始实现第一层了。\n",
    "\n",
    "它由一个卷积接一个max pooling完成。\n",
    "\n",
    "卷积在每个5x5的patch中算出32个特征。\n",
    "\n",
    "卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "\n",
    "而对于每一个输出通道都有一个对应的偏置量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了用这一层，我们把x变成一个4d向量，其第2、第3维对应图片的宽、高，最后一维代表图片的颜色通道数(因为是灰度图所以这里的通道数为1，如果是rgb彩色图，则为3)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_image = tf.reshape(x, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们把x_image和权值向量进行卷积，加上偏置项，然后应用ReLU激活函数，最后进行max pooling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层卷积\n",
    "为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接层\n",
    "现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "为了减少过拟合，我们在输出层之前加入dropout。\n",
    "我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。\n",
    "这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 \n",
    "TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-0eb814f49c23>:2: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出层\n",
    "最后，我们添加一个softmax层，就像前面的单层softmax regression一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和评估模型\n",
    "这个模型的效果如何呢？\n",
    "\n",
    "为了进行训练和评估，我们使用与之前简单的单层SoftMax神经网络模型几乎相同的一套代码，\n",
    "\n",
    "只是我们会用更加复杂的ADAM优化器来做梯度最速下降，\n",
    "\n",
    "在feed_dict中加入额外的参数keep_prob来控制dropout比例。\n",
    "\n",
    "然后每100次迭代输出一次日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "cross_entropy_map = tf.losses.softmax_cross_entropy(onehot_labels=y_, logits=y)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy_map)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert mnist_x.shape[0] == mnist_y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_placeholder = tf.placeholder(mnist_x.dtype, mnist_x.shape)\n",
    "labels_placeholder = tf.placeholder(mnist_y.dtype, mnist_y.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.repeat(20)\n",
    "dataset = dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: mnist_x, labels_placeholder: mnist_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........\n",
      "step 200, training accuracy 0.179688 \n",
      ".........\n",
      "step 400, training accuracy 0.218750 \n",
      ".........\n",
      "step 600, training accuracy 0.265625 \n",
      ".........\n",
      "step 800, training accuracy 0.226562 \n",
      ".........\n",
      "step 1000, training accuracy 0.226562 \n",
      ".........\n",
      "step 1200, training accuracy 0.218750 \n",
      ".........\n",
      "step 1400, training accuracy 0.210938 \n",
      ".........\n",
      "step 1600, training accuracy 0.242188 \n",
      ".........\n",
      "step 1800, training accuracy 0.265625 \n",
      ".........\n",
      "step 2000, training accuracy 0.359375 \n",
      ".........\n",
      "step 2200, training accuracy 0.304688 \n",
      ".........\n",
      "step 2400, training accuracy 0.351562 \n",
      ".........\n",
      "step 2600, training accuracy 0.234375 \n",
      ".........\n",
      "step 2800, training accuracy 0.117188 \n",
      ".........\n",
      "step 3000, training accuracy 0.312500 \n",
      ".........\n",
      "step 3200, training accuracy 0.265625 \n",
      ".........\n",
      "step 3400, training accuracy 0.289062 \n",
      ".........\n",
      "step 3600, training accuracy 0.250000 \n",
      ".........\n",
      "step 3800, training accuracy 0.281250 \n",
      ".........\n",
      "step 4000, training accuracy 0.273438 \n",
      ".........\n",
      "step 4200, training accuracy 0.281250 \n",
      ".........\n",
      "step 4400, training accuracy 0.359375 \n",
      ".........\n",
      "step 4600, training accuracy 0.367188 \n",
      ".........\n",
      "step 4800, training accuracy 0.421875 \n",
      ".........\n",
      "step 5000, training accuracy 0.382812 \n",
      ".........\n",
      "step 5200, training accuracy 0.453125 \n",
      ".........\n",
      "step 5400, training accuracy 0.437500 \n",
      ".........\n",
      "step 5600, training accuracy 0.468750 \n",
      ".........\n",
      "step 5800, training accuracy 0.406250 \n",
      ".........\n",
      "step 6000, training accuracy 0.468750 \n",
      ".........\n",
      "step 6200, training accuracy 0.484375 \n",
      ".........\n",
      "step 6400, training accuracy 0.453125 \n",
      ".........\n",
      "step 6600, training accuracy 0.476562 \n",
      ".........\n",
      "step 6800, training accuracy 0.492188 \n",
      ".........\n",
      "step 7000, training accuracy 0.578125 \n",
      ".........\n",
      "step 7200, training accuracy 0.453125 \n",
      ".........\n",
      "step 7400, training accuracy 0.351562 \n",
      ".........\n",
      "step 7600, training accuracy 0.484375 \n",
      ".........\n",
      "step 7800, training accuracy 0.515625 \n",
      ".........\n",
      "step 8000, training accuracy 0.429688 \n",
      ".........\n",
      "step 8200, training accuracy 0.492188 \n",
      ".........\n",
      "step 8400, training accuracy 0.617188 \n",
      ".........\n",
      "step 8600, training accuracy 0.562500 \n",
      ".........\n",
      "step 8800, training accuracy 0.546875 \n",
      ".........\n",
      "step 9000, training accuracy 0.609375 \n",
      ".........\n",
      "step 9200, training accuracy 0.539062 \n",
      "........End of dataset\n",
      "9376\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "step = 1\n",
    "try:\n",
    "    while True:\n",
    "        batch = sess.run(next_element, feed_dict={features_placeholder: mnist_x, labels_placeholder: mnist_y})\n",
    "        features = batch[0][:, :, :, np.newaxis]\n",
    "        label = batch[1]\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x: features , y_: label, keep_prob: 1.0})\n",
    "        #print(tf.argmax(y, 1).eval(feed_dict={x: features, y_: label, keep_prob: 1.0}))\n",
    "        #print(tf.argmax(y_, 1).eval(feed_dict={x: features, y_: label, keep_prob: 1.0}))\n",
    "        train_step.run(feed_dict={x: features, y_: label, keep_prob: 0.5})\n",
    "        \n",
    "        step = step + 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"\\nstep %d, training accuracy %f \" % (step, train_accuracy))\n",
    "        elif step % 20 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features , label = test\n",
    "features = features[:, :, :, np.newaxis]\n",
    "label = tf.keras.utils.to_categorical(label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy.eval(feed_dict={x: features , y_: label, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止，我们已经学会了用TensorFlow快捷地搭建、训练和评估一个复杂一点儿的深度学习模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.563"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 1, ..., 4, 8, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(y, 1).eval(feed_dict={x: features , y_: label, keep_prob: 1.0})\n",
    "#train_step.run(feed_dict={x: features, y_: label, keep_prob: 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(y_, 1).eval(feed_dict={x: features , y_: label, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
