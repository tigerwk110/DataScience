{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow API 中的tf.nn，tf.layers与tf.contrib\n",
    "\n",
    "我们在使用tensorflow时，会发现tf.nn，tf.layers， tf.contrib模块有很多功能是重复的，尤其是卷积操作。\n",
    "\n",
    "在使用的时候，我们可以根据需使用不同的模块也可以一起混用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.nn\n",
    "tf.nn ：提供神经网络相关操作的支持，包括卷积操作（conv）、池化操作（pooling）、归一化、loss、分类操作、embedding、RNN、Evaluation。\n",
    "\n",
    "## tf.layers\n",
    "定义在tensorflow/python/layers/layers.py, 为我们提供了一些高层次的构建神经网络的接口。\n",
    "今后会被放到keras模块中\n",
    "\n",
    "## tf.contrib\n",
    "tf.contrib.layers提供够将计算图中的 网络层、正则化、摘要操作、是构建计算图的高级操作，但是tf.contrib包含不稳定和实验代码。\n",
    "\n",
    "tF.Contrib，开源社区贡献，新功能，内外部测试，根据反馈意见改进性能，改善API友好度，API稳定后，移到TensorFlow核心模块。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/05_01.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以conv2d举例\n",
    "让我们看一下tf.nn.conv2d和tf.layers.conv2d之间的显着差异。\n",
    "\n",
    "后者处理权重和偏差所需的所有变量，单行代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/05_02.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于上一节的实例中，使用tf.nn.conv2d创建卷积层，必须在将权重变量传递给函数之前自己声明权重变量。\n",
    "然后自己需要添加对应激活函数。\n",
    "<img src=\"./images/05_03.png\"> <br>\n",
    "除此之外，tf.layers.conv2d还建议在同一个函数调用中添加正则化和激活，可以想象当更高级别的API覆盖了需求时，这可以减少代码大小.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 让我们使用高级API进行tensorflow模型的构建\n",
    "\n",
    "## 首先使用keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, mnist_y = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\"\"\"\n",
    "现在我们可以开始实现第一层了。\n",
    "它由一个卷积接一个max pooling完成。\n",
    "卷积在每个5x5的patch中算出32个特征。\n",
    "卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "而对于每一个输出通道都有一个对应的偏置量。\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\"\"\"\n",
    "# Layer 1\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
    "\n",
    "\"\"\"\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\"\"\"\n",
    "          \n",
    "# Layer 2\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPool2D(pool_size=(2, 2), padding='same', strides=(2, 2)))\n",
    "\n",
    "\"\"\"\n",
    "现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。\n",
    "我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\"\"\"\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              3212288   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,274,634\n",
      "Trainable params: 3,274,634\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.Model.compile` 采用三个重要参数：\n",
    "\n",
    "`optimizer`：此对象会指定训练过程。从 `tf.train` 模块向其传递优化器实例，例如 `tf.train.AdamOptimizer`、`tf.train.RMSPropOptimizer` 或 `tf.train.GradientDescentOptimizer`。\n",
    "`loss`：要在优化期间最小化的函数。常见选择包括均方误差 (mse)、categorical_crossentropy 和 binary_crossentropy。损失函数由名称或通过从 `tf.keras.losses` 模块传递可调用对象来指定。\n",
    "`metrics`：用于监控训练。它们是 tf.keras.metrics 模块中的字符串名称或可调用对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 37s 620us/sample - loss: 3.4827 - acc: 0.7586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 1.7814 - acc: 0.8740\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1.4871 - acc: 0.9006\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 1.4714 - acc: 0.9050\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 1.4650 - acc: 0.9070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b80f9eb38>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(mnist_x.reshape(60000, 28, 28, 1), tf.keras.utils.to_categorical(mnist_y, 10), epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先tf.layers中的层来构建我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "mnist_x, mnist_y = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-69aacb784037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \"\"\"\n\u001b[1;32m      3\u001b[0m \u001b[0m现在我们可以开始实现第一层了\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m它由一个卷积接一个max\u001b[0m \u001b[0mpooling完成\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m卷积在每个5x5的patch中算出32个特征\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "\"\"\"\n",
    "现在我们可以开始实现第一层了。\n",
    "它由一个卷积接一个max pooling完成。\n",
    "卷积在每个5x5的patch中算出32个特征。\n",
    "卷积的权重张量形状是[5, 5, 1, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "而对于每一个输出通道都有一个对应的偏置量。\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\"\"\"\n",
    "# Layer 1\n",
    "conv1 = layers.conv2d(inputs=x_image, filters=32, kernel_sie=(5, 5), activation='relu', padding='same')\n",
    "pool1 = layers.max_pooling2d(inputs=conv1, pool_size=(2, 2), padding='same', strides=(2, 2))\n",
    "\n",
    "\"\"\"\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\"\"\"\n",
    "          \n",
    "# Layer 2\n",
    "conv2 = layers.conv2d(inputs=pool1, filters=64, kernel_sie=(5, 5), activation='relu', padding='same')\n",
    "pool2 = layers.max_pooling2d(inputs=conv2, pool_size=(2, 2), padding='same', strides=(2, 2))\n",
    "\n",
    "\"\"\"\n",
    "现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。\n",
    "我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\"\"\"\n",
    "flatten = layers.flatten(inputs=pool2, name='flatten')\n",
    "fc1 = layers.dense(inputs=flatten, units=1024, activation='relu')\n",
    "fc2 = layers.dense(inputs=fc2, units=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
