{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Tensorflow (低级API)构建CNN对cifar10数据进行分类\n",
    "\n",
    "我们使用tf.nn的api对cifar10进行分析。\n",
    "构建如下结构的cnn，来对构建一个cifar10的分类模型。\n",
    "\n",
    "input: (None, 32, 32, 3)\n",
    "\n",
    "conv layer1: \n",
    "\n",
    "kernel size = (5, 5), filters = 32, strides = (1, 1, 1, 1)\n",
    "\n",
    "maxpooling layer1:\n",
    "\n",
    "strides = (1, 2, 2, 1)\n",
    "\n",
    "conv layer2:\n",
    "\n",
    "kernel size = (5, 5), filters = 64, strides = (1, 1, 1, 1)\n",
    "\n",
    "maxpooling layer2:\n",
    "\n",
    "strides = (1, 2, 2, 1)\n",
    "\n",
    "fc layer:\n",
    "\n",
    "filters=1024 \n",
    "\n",
    "softmax output layer\n",
    "filters = 10\n",
    "\n",
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "DATA_PATH = './data/cifar10'\n",
    "x_train = np.load(DATA_PATH + os.sep + 'x_train.npy')\n",
    "y_train = np.load(DATA_PATH + os.sep + 'y_train.npy')\n",
    "x_test = np.load(DATA_PATH + os.sep + 'x_test.npy')\n",
    "y_test = np.load(DATA_PATH + os.sep + 'y_test.npy')\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "images_and_labels = list(zip(x_train, y_train))\n",
    "plt.figure(figsize=(10,10), dpi=100)\n",
    "for index, (image, label) in enumerate(images_and_labels[:32]):\n",
    "    plt.subplot(4, 8, index + 1)\n",
    "    # 不显示坐标轴\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.title('%i' % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot coding\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建一个多层卷积网络\n",
    "\n",
    "我们尝试使用tensorflow的低级API构建一个CNN，来对cifar10进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权重初始化\n",
    "\n",
    "为了创建这个模型，我们需要创建大量的权重和偏置项。\n",
    "\n",
    "这个模型中的权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。\n",
    "\n",
    "由于我们使用的是ReLU神经元，因此比较好的做法是用一个较小的正数来初始化偏置项，以避免神经元节点输出恒为0的问题（dead neurons）。\n",
    "\n",
    "为了不在建立模型的时候反复做初始化操作，我们定义两个函数用于初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积和池化\n",
    "\n",
    "TensorFlow在卷积和池化上有很强的灵活性。我们怎么处理边界？步长应该设多大？\n",
    "\n",
    "在这个实例里，我们会一直使用vanilla版本。我们的卷积使用1步长（stride size），0边距（padding size）的模板，保证输出和输入是同一个大小。\n",
    "\n",
    "我们的池化用简单传统的2x2大小的模板做max pooling。\n",
    "\n",
    "为了代码更简洁，我们把这部分抽象成一个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w为输入\n",
    "# W为全集核\n",
    "def conv2d(x, W):\n",
    "    # strides在官方定义中是一个一维具有四个元素的张量，其规定前后必须为1，所以我们可以改的是中间两个数，\n",
    "    # 中间两个数分别代表了水平滑动和垂直滑动步长值。\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 占位符\n",
    "我们通过为输入图像和目标输出类别创建节点，来开始构建计算图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x为输入的数据，第一个维度为None\n",
    "x = tf.placeholder(\"float\", shape=[None, 32, 32, 3])\n",
    "# y_为输入数据对应的label\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一层卷积\n",
    "现在我们可以开始实现第一层了。\n",
    "\n",
    "它由一个卷积接一个max pooling完成。\n",
    "\n",
    "卷积在每个5x5的patch中算出32个特征。\n",
    "\n",
    "卷积的权重张量形状是[5, 5, 3, 32]，前两个维度是patch的大小，接着是输入的通道数目，最后是输出的通道数目。\n",
    "\n",
    "而对于每一个输出通道都有一个对应的偏置量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二层卷积\n",
    "为了构建一个更深的网络，我们会把几个类似的层堆叠起来。第二层中，每个5x5的patch会得到64个特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全连接层\n",
    "现在，图片尺寸减小到7x7，我们加入一个有1024个神经元的全连接层，用于处理整个图片。我们把池化层输出的张量reshape成一些向量，乘上权重矩阵，加上偏置，然后对其使用ReLU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "为了减少过拟合，我们在输出层之前加入dropout。\n",
    "我们用一个placeholder来代表一个神经元的输出在dropout中保持不变的概率。\n",
    "这样我们可以在训练过程中启用dropout，在测试过程中关闭dropout。 \n",
    "TensorFlow的tf.nn.dropout操作除了可以屏蔽神经元的输出外，还会自动处理神经元输出值的scale。所以用dropout的时候可以不用考虑scale。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, rate = 1-keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出层\n",
    "最后，我们添加一个softmax层，就像前面的单层softmax regression一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和评估模型\n",
    "这个模型的效果如何呢？\n",
    "\n",
    "我们仍然使用softmax作为输出。\n",
    "\n",
    "ADAM优化器来做梯度最速下降，\n",
    "\n",
    "在feed_dict中加入额外的参数keep_prob来控制dropout比例。\n",
    "\n",
    "然后每100次迭代输出一次日志。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_map = tf.losses.softmax_cross_entropy(onehot_labels=y_, logits=y)\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy_map)\n",
    "# 如果相等则返回True，不想等则返回False\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "# 计算平均值\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] == y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_placeholder = tf.placeholder(x_train.dtype, x_train.shape)\n",
    "labels_placeholder = tf.placeholder(y_train.dtype, y_train.shape)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.repeat(3)\n",
    "dataset = dataset.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = dataset.make_initializable_iterator()\n",
    "sess.run(iterator.initializer, feed_dict={features_placeholder: x_train, labels_placeholder: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_element = iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "step = 1\n",
    "try:\n",
    "    while True:\n",
    "        batch = sess.run(next_element, feed_dict={features_placeholder: x_train, labels_placeholder: y_train})\n",
    "        features = batch[0]\n",
    "        label = batch[1]\n",
    "        print(\"features' shape is\", features.shape)\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: features , y_: label, keep_prob: 1.0})\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: features, y_: label, keep_prob: 0.5})\n",
    "        \n",
    "        step = step + 1\n",
    "        if step % 200 == 0:\n",
    "            print(\"\\nstep %d, training accuracy %f \" % (step, train_accuracy))\n",
    "        elif step % 20 == 0:\n",
    "            print('.', end='')\n",
    "        \n",
    "except tf.errors.OutOfRangeError:\n",
    "    print(\"End of dataset\")\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = x_test\n",
    "label = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy.eval(feed_dict={x: features , y_: label, keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前为止，我们已经学会了用TensorFlow低级api中tf.nn api 搭建、训练和评估一个复杂一点儿的深度学习模型。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
